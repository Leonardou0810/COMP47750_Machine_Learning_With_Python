{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bce04c",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dce08c",
   "metadata": {},
   "source": [
    "1. Load the dataset into a dataframe that can be used for predicting traffic_volume a day in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv(\"metro_traffic_15_19.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ee36e",
   "metadata": {},
   "source": [
    "For the purpose of better training performance, we will implement some simple data cleaning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the duplicated instances\n",
    "print('Number of duplicate (excluding first) rows in the table is: ', df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the duplicated instances\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the 'date_time' into datetime64\n",
    "df[\"date_time\"] = df[\"date_time\"].astype(\"datetime64\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91638d0",
   "metadata": {},
   "source": [
    "2. Produce some plots at different time-scales to see if there is periodicity in the traffic volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882edb82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#In order to plot the trendancy of traffic volume based on different time-scales, we create a backup dataframe and make necessary adjustments for plotting.\n",
    "df_backup = df.copy()\n",
    "\n",
    "#Extract year, month, weekday and hour from 'date_time'\n",
    "df_backup[\"hour\"] = df_backup[\"date_time\"].dt.hour\n",
    "df_backup[\"year\"] = df_backup[\"date_time\"].dt.year\n",
    "df_backup[\"month\"] = df_backup[\"date_time\"].dt.month\n",
    "df_backup[\"weekday\"] = df_backup[\"date_time\"].dt.weekday + 1\n",
    "df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will plot the average traffic volumn of different years using hour(0-23) as base.\n",
    "#Create a new empty column.\n",
    "df_backup[\"avg_traffic_volume_byhour\"] = \"\"\n",
    "\n",
    "#Create lists to store all the unique values in 'hour' and 'year'.\n",
    "x_list = df_backup['hour'].unique().tolist()\n",
    "year_list = df_backup['year'].unique().tolist()\n",
    "\n",
    "for i in x_list:\n",
    "    for j in year_list:\n",
    "        #Calculate the average traffic volumn by hour.\n",
    "        df_backup[\"avg_traffic_volume_byhour\"].loc[(df_backup['hour']== i) & (df_backup['year']== j)] = df_backup['traffic_volume'].loc[(df_backup['hour']== i) & (df_backup['year']== j)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the df_backup\n",
    "df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'avg_traffic_volume_byhour' to 'float64' data type\n",
    "df_backup['avg_traffic_volume_byhour'] = df_backup['avg_traffic_volume_byhour'].astype(\"float64\")\n",
    "df_backup['year'] = df_backup['year'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the trendancy using lineplot.\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.lineplot(x=df_backup[\"hour\"], y=df_backup[\"avg_traffic_volume_byhour\"], hue=df_backup[\"year\"])\n",
    "ax.set(xticks=x_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da6251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Similarly, we plot the average traffic volumn of different years using month(1-12) as base.\n",
    "df_backup[\"avg_traffic_volume_bymonth\"] = \"\"\n",
    "month_list = df_backup['month'].unique().tolist()\n",
    "\n",
    "for i in month_list:\n",
    "    for j in year_list:\n",
    "        df_backup[\"avg_traffic_volume_bymonth\"].loc[(df_backup['month']== i) & (df_backup['year']== j)] = df_backup['traffic_volume'].loc[(df_backup['month']== i) & (df_backup['year']== j)].mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup[\"avg_traffic_volume_bymonth\"] = df_backup[\"avg_traffic_volume_bymonth\"].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.lineplot(x=df_backup[\"month\"], y=df_backup[\"avg_traffic_volume_bymonth\"], hue=df_backup[\"year\"])\n",
    "ax.set(xticks=month_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, we plot the average traffic volumn of different years using month(1-12) as base.\n",
    "df_backup[\"avg_traffic_volume_byweekday\"] = \"\"\n",
    "month_list = df_backup['weekday'].unique().tolist()\n",
    "\n",
    "for i in month_list:\n",
    "    for j in year_list:\n",
    "        df_backup[\"avg_traffic_volume_byweekday\"].loc[(df_backup['weekday']== i) & (df_backup['year']== j)] = df_backup['traffic_volume'].loc[(df_backup['weekday']== i) & (df_backup['year']== j)].mean()\n",
    "        \n",
    "df_backup[\"avg_traffic_volume_byweekday\"] = df_backup[\"avg_traffic_volume_byweekday\"].astype(\"float64\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.lineplot(x=df_backup[\"weekday\"], y=df_backup[\"avg_traffic_volume_byweekday\"], hue=df_backup[\"year\"])\n",
    "ax.set(xticks=month_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db46722",
   "metadata": {},
   "source": [
    "From the lineplots above, we can see that:\n",
    "1. Traffic volume varies according to the hour of the day. 3 am has the minimum traffic volum in a day, and 16 pm has the maximum traffic volumn. \n",
    "2. From 3 to 7, traffic volumn experiences a constant increase, and from 16 to 3 in the second day, traffic volumn experiences a constant decrease.\n",
    "3. Trend lines for different years show approximately identical trendancy.\n",
    "4. The average traffic volume fluctuates over different month of the year. Generally, November, December and Janurary has relatively low traffic volume, while March, June and August has high traffic volume.\n",
    "5. Monday to Friday have relativley high traffic volume on average and Saturday and Sunday have lower traffic volumn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83e1e6",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9ad49",
   "metadata": {},
   "source": [
    "1. Extract hour, day and month features from the time-stamps. Build two different regression models and test the accuracy. Try Linear Regression and one other regression model from scikit learn.\n",
    "\n",
    "2. Divide the data into train and test sets keeping one third of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract hour, day and month features from the time-stamps.\n",
    "df[\"hour\"] = df[\"date_time\"].dt.hour\n",
    "df[\"day\"] = df[\"date_time\"].dt.day\n",
    "df[\"month\"] = df[\"date_time\"].dt.month\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982dcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into train and test sets\n",
    "#We will set the 'traffic_valume' as dependant variable and 'rain_1h', 'snow_1h', 'temp', 'clouds_all', 'hour', 'year', 'day' as independant variables\n",
    "y = pd.DataFrame(df[\"traffic_volume\"])\n",
    "X = pd.DataFrame(df[['rain_1h', 'snow_1h', 'temp', 'clouds_all', 'hour', 'month', 'day']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3,random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05a141",
   "metadata": {},
   "source": [
    "<b> Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model fitting with the training set.\n",
    "linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of training set\n",
    "print(' R squared statistic of linear regression training set: {:.2f}'.format(linreg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction\n",
    "actual_vs_predicted = pd.concat([y_test, pd.DataFrame(linreg.predict(X_test), columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of testing set\n",
    "print(' R squared statistic of linear regression testing set: {:.2f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfc627",
   "metadata": {},
   "source": [
    "<b> Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1d1bd",
   "metadata": {},
   "source": [
    "Here I chose Decision Tree regression model to train the data. The optimal values for paramater ('max_depth' and 'random_state') are not taken into consideration in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Decision Tree model. \n",
    "dtr = DecisionTreeRegressor(max_depth=5, random_state=3)\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58964b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of training set\n",
    "print(' R squared statistic of decision tree training set: {:.2f}'.format(dtr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_vs_predicted = pd.concat([y_test, pd.DataFrame(dtr.predict(X_test), columns=['Predicted'])], axis=1)\n",
    "# print(actual_vs_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473142ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of testing set\n",
    "print(' R squared statistic of of decision tree training set: {:.2f}'.format(dtr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eebded",
   "metadata": {},
   "source": [
    "### Additional Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb33989",
   "metadata": {},
   "source": [
    "As we were given examples of standarlizing X_train and X_test in 'Regression tutorial' using `StandardScaler()`, in this section I will investigate on whether feature standarlization will impact the accuracy of the linear regression and decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarlize the X datasets\n",
    "X_train_s = StandardScaler().fit_transform(X_train)\n",
    "X_test_s = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model fitting with the standarlized training set.\n",
    "linreg_Xs = LinearRegression().fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of testing set\n",
    "print(' R squared statistic of linear regression testing set: {:.2f}'.format(linreg_Xs.score(X_test_s, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Decision Tree model with the standarlized training set.\n",
    "dtr_Xs = DecisionTreeRegressor(max_depth=5, random_state=3)\n",
    "dtr_Xs.fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ada2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the R squared of testing set\n",
    "print(' R squared statistic of of decision tree training set: {:.2f}'.format(dtr_Xs.score(X_test_s, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57664f19",
   "metadata": {},
   "source": [
    "From the results above we can see that feature Standarlization made no contribution to the improvement of accuracy for both of the linear regression and decision tree model. Therefore, we will no longer consider the feature transformation in the following steps in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ec211",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73ad48",
   "metadata": {},
   "source": [
    "1. Given that the linear numeric encoding of the hour, day and month features may miss cyclical signals, investigate and test a cyclical strategy for encoding these features. Does this strategy improve accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e2d7e",
   "metadata": {},
   "source": [
    "In this task, we will investigate cyclical encoding of the hour, day and month features with sine/cosine transformation. \n",
    "\n",
    "In task 2, we treat these features as continuous features and put them directly into the model trainning. However, one problem we ignored in task 2 about cyclical features was there are jump discontinuities in the graph at the end of each hour/day/month, for example when the hour value goes from  23  to  00.\n",
    "\n",
    "A common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n",
    "\n",
    "We can do that using the following transformations:\n",
    "\n",
    "𝑥𝑠𝑖𝑛=sin(2∗𝜋∗𝑥max(𝑥)) \n",
    "\n",
    "𝑥𝑐𝑜𝑠=cos(2∗𝜋∗𝑥max(𝑥))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define two functions to transform hour/day/month data into sine and consine values.\n",
    "#Code adopted from: https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/ \n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c09475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will copy the initial DataFrame, then encode hour/day/month columns using the sine/cosine transformations.\n",
    "df2 = df.copy()\n",
    "df2[\"hour_sin\"] = sin_transformer(24).fit_transform(df2[\"hour\"])\n",
    "df2[\"hour_cos\"] = cos_transformer(24).fit_transform(df2[\"hour\"])\n",
    "df2[\"day_sin\"] = sin_transformer(31).fit_transform(df2[\"day\"])\n",
    "df2[\"day_cos\"] = cos_transformer(31).fit_transform(df2[\"day\"])\n",
    "df2[\"month_sin\"] = sin_transformer(12).fit_transform(df2[\"month\"])\n",
    "df2[\"month_cos\"] = cos_transformer(12).fit_transform(df2[\"month\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will select the transformed hour/day/month features instead and re-train the linear regression model.\n",
    "y = pd.DataFrame(df2[\"traffic_volume\"])\n",
    "X = pd.DataFrame(df2[['rain_1h', 'snow_1h', 'temp', 'clouds_all', 'hour_sin','hour_cos', 'month_sin','month_cos','day_sin','day_cos']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3,random_state=3)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted = pd.concat([y_test, pd.DataFrame(linreg.predict(X_test), columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' R squared statistic of linear regression on training set: {:.2f}'.format(linreg.score(X_train, y_train)))\n",
    "print(' R squared statistic of linear regression on testing set: {:.2f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04ce9f",
   "metadata": {},
   "source": [
    "As we can see above, both the accuracies from training set and testing set experienced significant increases from around 0.15 shown in task 2 to 0.65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(max_depth=5, random_state=1)\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' R squared statistic of decision tree on training set: {:.2f}'.format(dtr.score(X_train, y_train)))\n",
    "print(' R squared statistic of decision tree on testing set: {:.2f}'.format(dtr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf6c83",
   "metadata": {},
   "source": [
    "However, when it comes to the results of decision tree, we can see that the accuracy based on the data after sine/cosine transformation makes no difference with the data before transformation. The accuracies for training/testing dataset are both around 0.79.\n",
    "\n",
    "Therefore, we believe that the cyclical strategy of sine/cosine transformation we implemented above for encoding the time feaures such as hour, day and month can significantly improve the accuracy of linear regression model. However, sine/cosine transformation makes minimal contribution for the improvement of performance from decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684da920",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f245ef5",
   "metadata": {},
   "source": [
    "1. Identify subsets of the features for this prediction task. These can be the same subset for all models or model-specific subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351d1ef",
   "metadata": {},
   "source": [
    "Information gain is used for determining the best features/attributes that render maximum information about a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f775f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from tutorial 10 Feature Selection\n",
    "mi = dict()\n",
    "\n",
    "i_scores = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "for i,j in zip(X.columns,i_scores):\n",
    "    mi[i]=j\n",
    " \n",
    "df_subset = pd.DataFrame.from_dict(mi,orient='index',columns=['I-Gain'])\n",
    "df_subset.sort_values(by=['I-Gain'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the IG of each features\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54caf7",
   "metadata": {},
   "source": [
    "According to the result in the 'df_subset', we can see that 'hour_cos' and 'hour_sin' are the features with high I-Gain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in df_subset.index.tolist():\n",
    "    #append i to the feature list\n",
    "    features.append(i)\n",
    "    #Split the trainning and testing sest\n",
    "    y = pd.DataFrame(df2[\"traffic_volume\"])\n",
    "    X = pd.DataFrame(df2[features])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3,random_state=3)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    #train the linear regression model\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    #Check the accuracy\n",
    "    print('Feature combination: ', features)\n",
    "    print(' R squared statistic of linear regression: {:.2f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the subset for decision tree model\n",
    "features = []\n",
    "for i in df_subset.index.tolist():\n",
    "    #append i to the feature list\n",
    "    features.append(i)\n",
    "    #Split the trainning and testing sest\n",
    "    y = pd.DataFrame(df2[\"traffic_volume\"])\n",
    "    X = pd.DataFrame(df2[features])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3,random_state=3)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    #train the linear regression model\n",
    "    dtr = DecisionTreeRegressor(max_depth=5, random_state=1)\n",
    "    dtr.fit(X_train, y_train)\n",
    "    #Check the accuracy\n",
    "    print('Feature combination: ', features)\n",
    "    print(' R squared statistic of decision tree: {:.2f}'.format(dtr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09535379",
   "metadata": {},
   "source": [
    "From the results above, we can conclude that 'hour_cos' and 'hour_sin' are the most improtant features.\n",
    "\n",
    "For linear regression model, the model accuracy reached to 0.65 after selecting 'hour_cos' and 'hour_sin' as input features and with later added features are selected, the accuracy shows subtle change.\n",
    "\n",
    "Decision Tree Regression model show similar situation, the only different is the feature selection of 'hour_cos', 'hour_sin', 'temp' gives a relativly high accuracy of 0.79, while after adding another feature 'clouds_all', the accuracy falls down to 0.78."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b6cba",
   "metadata": {},
   "source": [
    "### Additional Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9739700",
   "metadata": {},
   "source": [
    "In Task 1, we plotted the relationship between traffic volumn and weekday (the day of the week), and we found out that weekday has strong correlation to average traffic volumn. In this section we will try to consider weekday as the feature and see if it can provide any significant improvement on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ace1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 'weekday' feature and transform it using sin/cos transformer.\n",
    "df2[\"weekday\"] = df2[\"date_time\"].dt.weekday + 1\n",
    "df2[\"weekday_sin\"] = sin_transformer(7).fit_transform(df2[\"weekday\"])\n",
    "df2[\"weekday_cos\"] = cos_transformer(7).fit_transform(df2[\"weekday\"])\n",
    "\n",
    "#We will select 'hour_sin','hour_cos' based on the conclusion we have made, and includes weekday_sin and weekday_cos.\n",
    "y = pd.DataFrame(df2[\"traffic_volume\"])\n",
    "X = pd.DataFrame(df2[['hour_sin','hour_cos', \"weekday_sin\", \"weekday_cos\"]])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3,random_state=3)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the linear regression model\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "#Check the accuracy\n",
    "print(' R squared statistic of linear regression: {:.2f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70771a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(max_depth=5, random_state=1)\n",
    "dtr.fit(X_train, y_train)\n",
    "#Check the accuracy\n",
    "print(' R squared statistic of decision tree: {:.2f}'.format(dtr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8bdb",
   "metadata": {},
   "source": [
    "We can see that 'weekday' feature do provide an important contribution in accuracy improvement for both linear regression (increased from 0.65 to 0.69 on test set) and decision tree regression (increased from 0.78 to 0.92)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
